{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This session uses code from https://github.com/Lodifice/mfnf-pdf-export which is licensed under Apache License 2.0\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import collections\n",
    "import shelve\n",
    "\n",
    "def sha256(text):\n",
    "    return hashlib.sha256(text.encode(\"utf8\")).hexdigest()\n",
    "\n",
    "def stablehash(obj):\n",
    "    if callable(getattr(obj, \"_stablehash\", None)):\n",
    "        return obj._stablehash()\n",
    "    elif isinstance(obj, str):\n",
    "        return sha256(obj)\n",
    "    elif isinstance(obj, collections.abc.Sequence):\n",
    "        return sha256(\";\".join([stablehash(x) for x in obj]))\n",
    "    elif isinstance(obj, collections.abc.Mapping):\n",
    "        return stablehash([\"<\" + stablehash(k) + \";\" + stablehash(v) + \">\" for k, v in obj.items()])\n",
    "    else:\n",
    "        print(obj)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class CachedFunction:\n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "\n",
    "    def __call__(self, func):\n",
    "        def new_func(*args, **kwargs):\n",
    "            key = stablehash([func.__name__, args, kwargs])\n",
    "\n",
    "            if key in self.db:\n",
    "                return self.db[key]\n",
    "            else:\n",
    "                result = func(*args, **kwargs)\n",
    "\n",
    "                self.db[key] = result\n",
    "\n",
    "                return result\n",
    "\n",
    "        return new_func\n",
    "\n",
    "DB = shelve.open(\"cache.db\", \"c\", writeback=True)\n",
    "cached_function = CachedFunction(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    \"\"\"Removes the prefix `prefix` from string `text` in case it is present.\"\"\"\n",
    "    return text[len(prefix):] if text.startswith(prefix) else text\n",
    "\n",
    "def remove_suffix(text, suffix):\n",
    "    \"\"\"Removes the suffix `suffix` from string `text` in case it is present.\"\"\"\n",
    "    return text[:len(text)-len(suffix)] if text.endswith(suffix) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from functools import reduce\n",
    "from urllib.parse import quote\n",
    "\n",
    "def select_singleton(x):\n",
    "    return next(iter(x.values()))\n",
    "\n",
    "def merge(obj1, obj2):\n",
    "    \"\"\"Merges two objects depending of the type of the first argument.\n",
    "    >>> merge(None, 42)\n",
    "    42\n",
    "    >>> merge(None, None) is None\n",
    "    True\n",
    "    >>> merge([1, 2], [6, 7])\n",
    "    [1, 2, 6, 7]\n",
    "    >>> d = merge({ \"a\": 1, \"b\": 2}, {\"b\": 3, \"c\": 4})\n",
    "    >>> d == {\"a\": 1, \"b\": 3, \"c\": 4}\n",
    "    True\n",
    "    \"\"\"\n",
    "    if obj1 is None:\n",
    "        return obj2\n",
    "    elif isinstance(obj1, list):\n",
    "        return obj1 + obj2\n",
    "    elif isinstance(obj2, dict):\n",
    "        result = obj1.copy()\n",
    "        result.update(obj2)\n",
    "        return result\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def query_path(obj, path):\n",
    "    return reduce(lambda x, y: y(x) if callable(y) else x[y], path, obj)\n",
    "\n",
    "class MediaWikiAPI():\n",
    "    \"\"\"Implements an API for content stored on a MediaWiki.\"\"\"\n",
    "\n",
    "    def __init__(self, domain=\"de.wikibooks.org\", req=requests.Session()):\n",
    "        \"\"\"Initializes the object.\n",
    "        Arguments:\n",
    "        domain -- domain of the MediaWiki, e.g. `\"de.wikibooks.org\"`\n",
    "        req    -- an session object of the `request` framework\n",
    "        \"\"\"\n",
    "        self.domain = domain\n",
    "        self.req = req\n",
    "\n",
    "    def _stablehash(self):\n",
    "        return stablehash((self.__class__.__name__, self.domain))\n",
    "\n",
    "    @property\n",
    "    def _index_url(self):\n",
    "        \"\"\"Returns the URL to the server's `index.php` file.\"\"\"\n",
    "        return \"https://\" + self.domain + \"/w/index.php\"\n",
    "\n",
    "    @property\n",
    "    def _api_url(self):\n",
    "        \"\"\"Returns the URL to the server's `api.php` file.\"\"\"\n",
    "        return \"https://\" + self.domain + \"/w/api.php\"\n",
    "\n",
    "    @property\n",
    "    def _rest_api_url(self):\n",
    "        \"\"\"Returns the URL to the server's REST API endpoints.\"\"\"\n",
    "        return \"https://\" + self.domain + \"/api/rest_v1\"\n",
    "\n",
    "    def _index_call(self, params):\n",
    "        \"\"\"Make an HTTP request to the server's `index.php` file.\"\"\"\n",
    "        req = self.req.get(self._index_url, params=params)\n",
    "\n",
    "        req.raise_for_status()\n",
    "\n",
    "        return req.text\n",
    "\n",
    "    def _api_call(self, endpoint, data={}, domain=None):\n",
    "        \"\"\"Call an REST API endpoint.\"\"\"\n",
    "        if domain is None:\n",
    "            api_url = self._rest_api_url\n",
    "        else:\n",
    "            api_url = \"https://\" + domain + \"/api/rest_v1\"\n",
    "        \n",
    "        endpoint_url = \"/\".join([api_url] + endpoint)\n",
    "\n",
    "        result = self.req.get(endpoint_url, data=data)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def query(self, params, path_to_result):\n",
    "        params[\"format\"] = \"json\"\n",
    "        params[\"action\"] = \"query\"\n",
    "        path_to_result = [\"query\"] + path_to_result\n",
    "        result = None\n",
    "\n",
    "        while True:\n",
    "            api_result = self.req.get(self._api_url, params=params).json()\n",
    "\n",
    "            if \"error\" in api_result:\n",
    "                message = \"Error while making API call.\"\n",
    "\n",
    "                raise ConnectionError(api_result.get(\"info\", message))\n",
    "\n",
    "            result = merge(result, query_path(api_result, path_to_result))\n",
    "\n",
    "            if \"continue\" in api_result:\n",
    "                params.update(api_result[\"continue\"])\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "    @cached_function\n",
    "    def get_revisions(self, title):\n",
    "        params = {\"prop\": \"revisions\", \"rvprop\": \"size|user|timestamp|userid|ids|comment\", \"titles\": title,\n",
    "                  \"rvlimit\": \"max\"}\n",
    "\n",
    "        try:\n",
    "            return self.query(params, [\"pages\", select_singleton, \"revisions\"])\n",
    "        except KeyError as e:\n",
    "            if str(e) == \"'revisions'\":\n",
    "                return []\n",
    "    \n",
    "    @cached_function\n",
    "    def get_all_pageviews(self, title):\n",
    "        endpoint = [\"metrics\", \"pageviews\", \"per-article\", self.domain, \"all-access\", \"user\",\n",
    "                    quote(title, safe=\"\"), \"daily\", \"20150701\", \"20171006\"]\n",
    "        \n",
    "        req = self._api_call(endpoint, domain=\"wikimedia.org\")\n",
    "        \n",
    "        if req.status_code == requests.codes.ok:\n",
    "            return req.json()[\"items\"]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_content(self, title):\n",
    "        return self._index_call({\"action\": \"raw\", \"title\": title})\n",
    "    \n",
    "    def pageviews_of(self, title, start, end):\n",
    "        if len(end) == 8:\n",
    "            end += \"00\"\n",
    "            \n",
    "        return sum((x[\"views\"] for x in self.get_all_pageviews(title) if x[\"timestamp\"] >= start and x[\"timestamp\"] <= end))\n",
    "    \n",
    "    def pageviews(self, title, start, end):\n",
    "        return sum((self.pageviews_of(x, start, end) for x in self.all_titles(title)))\n",
    "    \n",
    "    def all_titles(self, title):\n",
    "        \"\"\"Returns a set of all titles the article `title` had in the past.\"\"\"\n",
    "        result = set()\n",
    "\n",
    "        result.add(title)\n",
    "\n",
    "        re_link = \"\\\\[\\\\[([^\\\\]]+)\\\\]\\\\]\"\n",
    "        re1 = \".*verschob die Seite %s nach %s.*\" % (re_link, re_link)\n",
    "        re2 = \".*hat „%s“ nach „%s“ verschoben.*\" % (re_link, re_link)\n",
    "        regs = [ re.compile(re1), re.compile(re2) ]\n",
    "\n",
    "        for comment in (x[\"comment\"] for x in self.get_revisions(title)):\n",
    "            for reg in regs:\n",
    "                m = reg.match(comment)\n",
    "\n",
    "                if m:\n",
    "                    result.add(m.group(1))\n",
    "                    result.add(m.group(2))\n",
    "\n",
    "        return result\n",
    "    \n",
    "wb = MediaWikiAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sitemap import parse_sitemap\n",
    "\n",
    "mfnf = parse_sitemap(wb.get_content(\"Mathe für Nicht-Freaks: Sitemap\"))\n",
    "mfnf[\"title\"] = \"Mathe für Nicht-Freaks\"\n",
    "analysis1 = sitemap[\"children\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seitenaufrufe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571766"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pageviews(node, start, end):\n",
    "    result = wb.pageviews(node[\"title\"], start, end) if node[\"title\"] else 0\n",
    "    \n",
    "    return result + sum((pageviews(x, start, end) for x in node[\"children\"]))\n",
    "\n",
    "pageviews(analysis1, \"20161001\", \"20170331\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
